# -*- coding: utf-8 -*-
"""Tindy_AI_server_final_ver.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jdBqX6gC3NfCzdYU4RVsxpxAMevIrXSx
"""

!pip install fastapi uvicorn requests python-multipart pillow diffusers rembg pyngrok faiss-cpu PEFT

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!ngrok authtoken 2ie9iO1rVne4PcnZlQgskGUCj3J_2kHdcgwHDRumgSm2j7EiB

from pyngrok import ngrok

import os
import logging
from fastapi import FastAPI, UploadFile, File, HTTPException, APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from queue import Queue, Empty
import threading
import asyncio
import uuid
from PIL import Image
from io import BytesIO
import torch
import uvicorn
import json
from logging.handlers import RotatingFileHandler
import json
from diffusers import StableDiffusionXLPipeline
from transformers import CLIPTokenizer, CLIPTextModel
import pickle
import faiss
import numpy as np
from tqdm import tqdm
from rembg import remove, new_session

import nest_asyncio

nest_asyncio.apply()

class Prompt(BaseModel):
    text_prompt: str
    style: str

class PromptRecommendInput(BaseModel):
    user_prompt: str
    recommend_size: int

class GenerationManager():
    def __init__(self, max_batch_size, prompt_prefix_path, model_path, lora_path, lora_file_name):
        self.logger = logging.getLogger("GenerationManager")

        # Set up rotating logs with a maximum file size and backup count
        log_handler = RotatingFileHandler(
            'generation_manager.log',  # Log file path
            maxBytes=5 * 1024 * 1024,  # 5 MB per log file
            backupCount=5  # Keep up to 5 backup log files
        )

        # Set the logging level and formatter for this logger
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        log_handler.setFormatter(formatter)
        self.logger.setLevel(logging.INFO)
        self.logger.addHandler(log_handler)

        self.events = {}
        self.img_gen_results = {}
        self.image_generation_queue = Queue()

        self.max_batch_size = max_batch_size

        with open(prompt_prefix_path, "r") as f:
            self.prompt_prefix = json.load(f)
            self.logger.info("[Controller] Prompt prefix loaded successfully.")

        try:
            self.img_gen_pipe = ImageGenerationPipeline(model_path, lora_path, lora_file_name)
            self.logger.info("[Controller] SDXL pipeline loaded successfully.")
        except Exception as e:
            self.logger.error(f"[Controller] Error initializing SDXL pipeline: {e}")

        self.logger.info("[Controller] Starting background thread for image generation...")
        self.background_thread = threading.Thread(target=self.batch_image_generation, daemon=True)
        self.background_thread.start()

    def batch_image_generation(self):
        self.logger.info("[Batch] Background thread started for image generation...")

        while True:
            temp_list = []
            # Collect all available requests, up to the batch size
            try:
              for _ in range(self.max_batch_size):
                  temp_list.append(self.image_generation_queue.get(block=False))
                  self.logger.info(f"[Batch] Got a request. Queue size: {self.image_generation_queue.qsize()}")
            except Empty:
              pass

            if not temp_list:
              continue

            # Process the prompts
            request_id_list = [id_prompt_tuple[0] for id_prompt_tuple in temp_list]
            try:
                prompt_list = ["Sticker, " + id_prompt_tuple[1].text_prompt if id_prompt_tuple[1].style == "sticker" else id_prompt_tuple[1].text_prompt + self.prompt_prefix[id_prompt_tuple[1].style] for id_prompt_tuple in temp_list]
            except Exception as e:
                self.logger.error(f"[Batch] Error processing prompts: {e}")
                for request_id in request_id_list:
                    self.img_gen_results[request_id] = None
                    if request_id in self.events:
                        self.events[request_id].set()  # Avoid deadlock
                raise HTTPException(status_code=503, detail="Invalid style.")

            self.logger.info(f"[Batch] Processing batch of size {len(temp_list)}...")

            try:
                # Reduce inference steps to speed up generation
                generated_images = self.img_gen_pipe.generate_image(prompt_list)
                torch.cuda.empty_cache()
                #generated_images = self.sdxl_refiner(prompt_list, image=unrefined_images, num_inference_steps=5).images
                for prompt in prompt_list:
                    self.logger.info(f"[Batch] Generated image for prompt: {prompt}")
                self.logger.info("[Batch] Images generated successfully.")
            except Exception as e:
                self.logger.error(f"[Batch] Error during image generation: {e}")
                # Handle the error and set the events to avoid deadlock
                for request_id in request_id_list:
                    self.img_gen_results[request_id] = None
                    if request_id in self.events:
                        self.events[request_id].set()  # Avoid deadlock
                continue

            # Store the results and set events
            for request_id, img in zip(request_id_list, generated_images):
                self.img_gen_results[request_id] = img
                if request_id in self.events:
                    self.events[request_id].set()  # Signal the corresponding thread
                    self.logger.info(f"[Batch] Image ready for request {request_id}")

    async def image_generation_response(self, request_id, prompt):
        self.logger.info(f"[Request] Received image generation request {request_id} with prompt: {prompt.text_prompt}")
        event = threading.Event()
        self.events[request_id] = event

        # Add the prompt to the queue
        self.image_generation_queue.put((request_id, prompt))
        self.logger.info(f"[Request] Added request {request_id} to the queue.")
        self.logger.info(f"[Request] Queue size: {self.image_generation_queue.qsize()}")

        # Wait until the image is generated
        await asyncio.to_thread(event.wait)
        self.logger.info(f"[Request] Image generated for request {request_id}")

        # Once the image is ready, retrieve it
        image = self.img_gen_results.get(request_id)

        # Ensure we have an image result
        if image is None:
            raise HTTPException(status_code=503, detail=f"Image generation failed. Please try again later.")

        # Convert the image to an in-memory file (BytesIO)
        img_io = BytesIO()
        image.save(img_io, format="JPEG")
        img_io.seek(0)  # Move the cursor to the start of the file

        # Clean up
        del self.img_gen_results[request_id]
        del self.events[request_id]
        self.logger.info(f"[Request] Image ready for request {request_id} as in-memory stream")
        return img_io

class ImageGenerationPipeline():
    def __init__(self, model_path, lora_path, lora_file_name):
        self.img_gen_pipe = StableDiffusionXLPipeline.from_pretrained(
            model_path, # "stabilityai/sdxl-turbo"
            variant="fp16",
            torch_dtype=torch.float16
        )
        self.img_gen_pipe.to("cuda")
        self.img_gen_pipe.load_lora_weights(lora_path, weight_name=lora_file_name)

    def generate_image(self, prompt_list):
        generated_images = self.img_gen_pipe(prompt_list, num_inference_steps=2, guidance_scale=1.0).images

        return generated_images

class PromptRecommender():
    def __init__(self, tokenizer_path, text_encoder_path, cluster_prompt_df_dict_path, cluster_keywords_scores_center_dict_path):
        self.logger = logging.getLogger("PromptRecommender")

        # Set up rotating logs with a maximum file size and backup count
        log_handler = RotatingFileHandler(
            'prompt_recommender.log',  # Log file path
            maxBytes=5 * 1024 * 1024,  # 5 MB per log file
            backupCount=5  # Keep up to 5 backup log files
        )

        # Set the logging level and formatter for this logger
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        log_handler.setFormatter(formatter)
        self.logger.setLevel(logging.INFO)
        self.logger.addHandler(log_handler)

        self.tokenizer = CLIPTokenizer.from_pretrained(tokenizer_path)
        self.logger.info("[Prompt_recommender] Tokenizer Loaded.")
        self.text_encoder = CLIPTextModel.from_pretrained(text_encoder_path, torch_dtype=torch.float16, variant="fp16")
        self.logger.info("[Prompt_recommender] Text Encoder Loaded.")

        with open(cluster_prompt_df_dict_path, 'rb') as f:
            self.cluster_prompt_df_dict = pickle.load(f)
            self.logger.info("[Prompt_recommender] Cluster Pompt Data Loaded.")

        with open(cluster_keywords_scores_center_dict_path, 'rb') as f:
            self.cluster_keywords_scores_center = pickle.load(f)
            self.logger.info("[Prompt_recommender] Cluster Keyword Data Loaded.")

        self.device = 'cpu'
        self.text_encoder.to(self.device)

        if self.device == 'cuda':
            self.gpu_res = faiss.StandardGpuResources()
        else:
            self.gpu_res = None


        self.cluster_centers = [None]*len(self.cluster_keywords_scores_center)
        for cluster_id, (cluster_keywords, cluster_center) in tqdm(self.cluster_keywords_scores_center.items(), desc="Comparing prompt with cluster centers"):
          if type(cluster_center) != np.ndarray:
            cluster_center = np.array(cluster_center, dtype=np.float32)
          elif cluster_center.dtype != np.float32:
            cluster_center = cluster_center.astype(np.float32)

          self.cluster_centers[cluster_id] = cluster_center

        self.embedding_dimenion = 768
        if self.gpu_res != None:
            self.index = faiss.index_cpu_to_gpu(self.gpu_res, 0, faiss.IndexFlatIP(self.embedding_dimenion))
        else:
            self.index = faiss.IndexFlatIP(self.embedding_dimenion)

        self.cluster_centers = np.array(self.cluster_centers, dtype=np.float32)

        faiss.normalize_L2(self.cluster_centers)
        self.index.add(self.cluster_centers)

        self.logger.info("[Prompt_recommender] PromptRecommender initialized.")

    def prompt_recommend_response(self, prompt_recommend_input):
        def embedding_user_prompt(prompt):
            # Tokenize the prompt
            tokens = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            tokens = tokens.to(self.device)

            with torch.no_grad():
                # Encode the prompt
                embedding = self.text_encoder(**tokens).pooler_output

            embedding = embedding.cpu().numpy().astype(np.float32)
            embedding = embedding / np.linalg.norm(embedding)


            return embedding

        def find_n_clusters(embedding_tensor, n):
            _, I =  self.index.search(embedding, n)

            return I[0].tolist()

        def find_key_phrase(embedding, cluster_ids):
            prompt_key_phrases = []
            for cluster_id in cluster_ids:
                hightest_sim = 0
                most_similar_prompt = None
                prompt_embedding_df = self.cluster_prompt_df_dict[cluster_id]
                keyword_score_list = self.cluster_keywords_scores_center[cluster_id][0]

                for prompt, prompt_embedding in zip(prompt_embedding_df["prompt"], prompt_embedding_df["embedding"]):
                    dot_product = np.dot(embedding, prompt_embedding)
                    norm_A = np.linalg.norm(embedding)
                    norm_B = np.linalg.norm(prompt_embedding)
                    cosine_similarity = dot_product / (norm_A * norm_B)

                    if cosine_similarity > hightest_sim:
                        hightest_sim = cosine_similarity
                        most_similar_prompt = prompt

                key_phrase = None
                highest_score = 0

                for phrase in most_similar_prompt.replace(".", ",").replace("!", ",").replace("?", ",").split(","):
                    score_sum = 0
                    for keyword, score in keyword_score_list:
                        if keyword in phrase:
                            score_sum += score
                    if score_sum > highest_score:
                        highest_score = score_sum
                        key_phrase = phrase

                prompt_key_phrases.append(key_phrase)

            return prompt_key_phrases

        self.logger.info(f"[Prompt_recommender] Received prompt recommendation request with prompt: {prompt_recommend_input.user_prompt}")
        embedding = embedding_user_prompt(prompt_recommend_input.user_prompt)
        cluster_ids = find_n_clusters(embedding, prompt_recommend_input.recommend_size)
        prompt_key_phrases = find_key_phrase(embedding, cluster_ids)
        self.logger.info(f"[Prompt_recommender] Successfully recommended styles for {prompt_recommend_input.user_prompt}.")

        return prompt_key_phrases

class BackgroundRemover():
    def __init__(self):
        self.logger = logging.getLogger("Controller")

        # Set up rotating logs with a maximum file size and backup count
        log_handler = RotatingFileHandler(
            'controller.log',  # Log file path
            maxBytes=5 * 1024 * 1024,  # 5 MB per log file
            backupCount=5  # Keep up to 5 backup log files
        )

        # Set the logging level and formatter for this logger
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        log_handler.setFormatter(formatter)
        self.logger.setLevel(logging.INFO)
        self.logger.addHandler(log_handler)
        self.session  = new_session()

    async def background_remove_response(self, file):
        # Log the start of the background removal process
        self.logger.info(f"[API] Received /remove-background request with file: {file.filename}, type: {file.content_type}")

        # Ensure the uploaded file is an image
        if not file.content_type.startswith("image/"):
            self.logger.error(f"[API] Invalid file type for background removal: {file.content_type}")
            raise HTTPException(status_code=400, detail="Invalid file type. Only image files are allowed.")

        try:
            # Read the uploaded file as bytes
            supported_formats = ["jpg","jpeg", "png", "bmp"]
            image_bytes = await file.read()

            # Open the image as a PIL image object
            input_image = Image.open(BytesIO(image_bytes))
            self.logger.info(f"[API] Image {file.filename} loaded successfully for background removal.")

            if input_image.format.lower() not in supported_formats:
                raise HTTPException(status_code=415, detail="Unsupported image format.")

            # Remove the background (returns a new image)
            output_image = remove(input_image, session=self.session)

            # Convert RGBA to RGB if needed
            if output_image.mode != 'RGBA':
                output_image = output_image.convert('RGBA')
                self.logger.info(f"[API] Image {file.filename} is not RGBA.")

            # Save the result to a BytesIO object
            output_buffer = BytesIO()
            output_image.save(output_buffer, format='PNG')
            output_buffer.seek(0)  # Reset buffer pointer to the beginning

            self.logger.info(f"[API] Successfully removed background from {file.filename}.")

            return output_buffer

        except Exception as e:
            # Log the error and raise an HTTPException
            self.logger.error(f"[API] Error occurred during background removal for {file.filename}: {e}")
            raise HTTPException(status_code=503, detail="Background removal failed. Please try again later.")

class Controller():
    def __init__(self):
        # Configure logger for the Controller instance
        self.logger = logging.getLogger("Controller")

        # Set up rotating logs with a maximum file size and backup count
        log_handler = RotatingFileHandler(
            'controller.log',  # Log file path
            maxBytes=5 * 1024 * 1024,  # 5 MB per log file
            backupCount=5  # Keep up to 5 backup log files
        )

        # Set the logging level and formatter for this logger
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        log_handler.setFormatter(formatter)
        self.logger.setLevel(logging.INFO)
        self.logger.addHandler(log_handler)

        # Initialize controller attributes
        self.logger.info("[Controller] Initializing the pipeline...")

        with open("/content/drive/MyDrive/Tindy_AI_Server/config.json", "r") as f:
            self.config = json.load(f)
            self.logger.info("[Controller] Config loaded successfully.")

        self.prompt_prefix_file = self.config["prompt_prefix_path"]

        with open(self.prompt_prefix_file, "r") as f:
            self.prompt_prefix = json.load(f)
            self.logger.info("[Controller] Prompt prefix loaded successfully.")

        try:
            # Initialize your prompt recommender with the tokenizer and encoder
            self.prompt_recommender = PromptRecommender(
                self.config["tokenizer_path"],
                self.config["text_encoder_path"],
                self.config["cluster_prompt_df_dict_path"],
                self.config["cluster_keywords_scores_center_path"]
            )


            self.logger.info("[Controller] Prompt Recommender init successfully.")

        except Exception as e:
            self.logger.error(f"[Controller] Error initializing prompt recommender: {e}")

        try:
            # Initialize your background remover
            self.background_remover = BackgroundRemover()
            self.logger.info("[Controller] Background Remover init successfully.")
        except Exception as e:
            self.logger.error(f"[Controller] Error initializing background remover: {e}")

        try:
            # self.img_gen_pipe = StableDiffusionXLPipeline.from_pretrained(
            #     self.config["sdxl_path"], # "stabilityai/sdxl-turbo"
            #     variant="fp16",
            #     torch_dtype=torch.float16
            # )
            # self.img_gen_pipe.to("cuda")
            # self.img_gen_pipe.load_lora_weights(self.config["lora_path"], weight_name=self.config["lora_file_name"])
            self.generation_manager = GenerationManager(self.config["max_batch_size"], self.config["prompt_prefix_path"], self.config["sdxl_path"], self.config["lora_path"], self.config["lora_file_name"])
            self.logger.info("[Controller] Generation Manager init successfully.")

        except Exception as e:
            self.logger.error(f"[Controller] Error initializing SDXL pipeline: {e}")

    async def image_generation_response(self, request_id, prompt):
        return await self.generation_manager.image_generation_response(request_id, prompt)

    async def background_remove_response(self, file):
        return await self.background_remover.background_remove_response(file)

    async def prompt_recommend_response(self, prompt):
        return self.prompt_recommender.prompt_recommend_response(prompt)

controller = Controller()
app = FastAPI()

@app.post("/generate-image")
async def generate_image(prompt: Prompt):
    # Generate a unique identifier for each request
    request_id = str(uuid.uuid4())
    controller.logger.info(f"[API] Received /generate-image request {request_id}")

    img_io = await controller.image_generation_response(request_id, prompt)

    # Return the image directly from memory using StreamingResponse
    return StreamingResponse(img_io, media_type="image/jpeg")

@app.post("/remove-background")
async def remove_background(file: UploadFile = File(...)):
    # Log the start of the prompt recommendation process
    request_id = str(uuid.uuid4())
    controller.logger.info(f"[API] Received /remove-background request, id : {request_id}")

    output_buffer = await controller.background_remove_response(file)
    controller.logger.info(f"[API] Successfully response /remove-background request, id : {request_id}")

    return StreamingResponse(output_buffer, media_type="image/png")

@app.post("/prompt-recommend")
async def prompt_recommend(prompt: PromptRecommendInput):
    # Log the start of the prompt recommendation process
    controller.logger.info(f"[API] Received /prompt_recommend request with prompt: {prompt.user_prompt}")

    key_phrases = controller.prompt_recommend_response(prompt)
    controller.logger.info(f"[API] Successfully recommended styles for {prompt.user_prompt}.")

    return key_phrases

if __name__ == "__main__":
    public_url = ngrok.connect(8000)
    print("Public URL:", public_url)
    uvicorn.run(app, host="0.0.0.0", port=8000)